\section{Block Sparse K-SVD algorithm applied to BSS}
Last section proves the strength of sparsity based methods applied to blind source separation problem. Inspired by using adaptively learned local dictionary in MMCA/GMCA, we aim to create a new adaptive dictionary learning algorithm combined with BSS. The idea driven behind is simple that, certainly will we acquire a better separation result if we improve the level of sparsity of the dictionary,.\\

A variety of sparse dictionary learnging algorithm have been proposed in the literature for this purpose, based on the K-SVD algorithm. The block-sparse dictionary learning algorithm is proposed by Lihi in \cite{dictionary_block_sparse}. It exploits the hidden structure that is intrinsic in the signals for producing more efficient sparse representations. In the following content, we introduce the theory of this algorithm and try to embed it in the blind source separation process. The algorithm consists of two steps: a block structure update step (SAC) and a dictionary update step (BK-SVD). \\

\subsection{Problem definition}
Given a set of signals $\mathbf{Y} = \{y_i\} \in R^N$, we wish to find an overcomplete dictionary $\mathbf{\Phi}$ in $R^{N \times K}$ whose atoms are sorted in blocks, correspondingly the non-zero coefficients representations $\mathbf{X} = \{x_i\}$ are concentrated in a fixed number of blocks. More specifically, suppose dictionary atoms sorted in blocks that enable \textbf{block-sparse} representations of input signals. Each block has its own label, indexed as $d_i$. We claim that a vector $\mathbf{X}\in R^K$ is $k$-block-sparse over $d$ if its non-zero entries are concentrated in $k$ blocks only. This is denoted by 
\begin{equation}
    ||x||_{0,d} = k
\end{equation}
which means there are $k$ number of non-zero blocks having block structure $d$. Figure \ref{block_dict_compare} presents two equivalent examples, The dictionary can be expressed as 5 blocks but with 2-block-sparse presentations.\\

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\textwidth]{images/block_dict_compare.png}
\caption{Two equivalent representations with different block structure.}
\label{block_dict_compare}
\end{figure}

We can now formulate the problem is a mathematical way that we intent to find a dictionary $\mathbf{\Phi}$ and a block structure d, with maximal block size s, that lead to optimal k-block sparse representations $\mathbf{X} = \{x_i\}^L_i $ for signals in $\mathbf{Y}$. The objective function is given as below.\\
\begin{equation}
\begin{aligned} 
    \min_{D,d,\mathbf{X}} ||\mathbf{Y} -\mathbf{\Phi}\mathbf{X}||\\
    \text{s.t.} \quad ||x_i||_{0,d} \leq k, \; i = 1,...L\\
    |d_j| \leq s, \; j = 1,....B
    \label{bskvsd_1}
\end{aligned}
\end{equation}

where $d_j$ is the set of indices belonging to block $j$, $s$ is the maximum block size and $B$ is
is the total number of blocks (number of dictionary columns divided by the block size). When the maximal block size is set to 1, the proposed algorithm reduces to normal K-SVD.\\


\subsection{Algorithm Preview}
Like other optimisation problems in previous sections, the problem in Equation (\ref{bskvsd_1}) is non-convex. We therefore adopt the coordinate relaxation technique. The initial dictionary can be set up as a DCT dictionary or any random collection of $K$ signals. Then the block structure is solved by the \textit{sparse aggolmerative clustering} (SAC) algorithm\cite{Johnson1967}. Agglomerative clustering is a `bottom-up' approach who groups according to distance metric (i.e. $\ell_0$ norm). SAC algorithm solves for an optimal block structure refer to input command ($k$ and $s$) while keeping the dictionary fixed.

\begin{equation}
\begin{aligned}
    \left[d^{(m)}, \;\mathbf{\Phi}^{(m)}\right] = \text{arg} \; \min_{X,d} \quad || \mathbf{Y} - \mathbf{\Phi}^{(m-1)}\mathbf{X}||_2 \\
    \text{s.t.} \quad ||x_i||_0,d \leq k, \quad i = 1,...,L\\
    |d_j| \leq s, \; j = 1,....B
\end{aligned}
\end{equation}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{images/flow_chart.png}
\caption{(a) A flow chart about the SAC algorithm; (b) A stepwise example of the SAC algorithm.}
\label{flow_sac}
\end{figure}

Figure (\ref{flow_sac}) presents a detailed example of the clustering process of the SAC algorithm. In this example, the maximal block size is set to be 2. At the first iteration, row 1 and row 3 in the dictionary has the largest intersection, by which means their $\ell_0$ norm has the largest similarity ($\omega = \{1,4\}$). Consequently, they are merged together. In the next step, row 2 and row 4 have the largest intersection and are merged. Up to now, no acquisitions an be further executed because of the maximal block size is limited by 2. \\

One may wonder that the SAC algorithm only deals with the sparse coefficients but not the dictionary directly. This is because rows of the coefficients $\mathbf{X}$ exhibits a similar pattern on non-zeros as the columns of the dictionary block. In other words, grouping coefficients is equivalent to grouping the dictionary atoms according to the sparsity pattern. Furthermore, merging blocks is always beneficial to the reconstruction accuracy of one dictionary, due to the reconstruction result is a liner summation of atoms, grouping only rewrite the coefficients in $\mathbf{X}$, which does not deprave the final result.\\


After having successfully determined the optimal block structure. We then solve for new dictionary while keeping $\mathbf{\Phi}^{m}$ fixed. The objective function becomes.
\begin{equation}
\begin{aligned}
    \left[\mathbf{X}^{(m)}, \;\mathbf{\Phi}^{(m)}\right] = \text{arg} \; \min_{\Phi,X} \quad || \mathbf{Y} - \mathbf{\Phi}^{(m-1)}\mathbf{X}||_2 \\
    \text{s.t.} \quad ||x_i||_0,d \leq k, \quad i = 1,...,L
\end{aligned}
\label{BKSVD1}
\end{equation}
The author in \cite{dictionary_block_sparse} proposed block K-SVD (BK-SVD), a natural extension of the K-SVD algorithm to solve (\ref{BKSVD1}). BK-SVD algorithm employs a similar `columnwise' atom updating manner as in K-SVD but forces the learned dictionary to have higher sparsity level. We first fix $\mathbf{\Phi}^{m-1}$ and can use any pursuit method (e.g. Batch-OMP) to solve (\ref{BKSVD1}) which reduces to.

\begin{equation}
\begin{aligned}
    \mathbf{X}^{(m)} = \text{arg} \; \min_X \quad || \mathbf{Y} - \mathbf{\Phi}\mathbf{X}^{(m-1)}||_2 \\
    \text{s.t.} \quad ||x_i||_0,d \leq k, \quad i = 1,...,L
\end{aligned}
\end{equation}

Then, to obtain $\mathbf{\Phi}^{(m)}$, fix $\mathbf{X}^{(m)}$, $d$ and $\mathbf{Y}$. The calculation procedure is same as K-SVD, where the blocks of atoms (not standalone atoms) in the dictionary is updated sequentially, alongside with the corresponding nonzero coefficients in $\mathbf{X}^{(m)}$. The key difference between K-SVD and BK-SVD is that, in K-SVD only the highest rank component of the residual is updated, resulting in one atom change. Conversely in BK-SVD, atoms in the same block can be updated simultaneously.\\

We have now introduced the whole block sparse dictionary learning algorithm. Extend the idea of K-SVD+MMCA algorithm, we can replace the step of calculating sparse coefficients and dictionary atoms by the newly proposed SAC+BK-SVD algorithm. The overall BSS framework using SAC+BK-SVD is summarised below.

\begin{algorithm}[!htbp] 
\caption{The numerical algorithm for SAC+BK-SVD+MMCA} 
\label{alg:Framwork} 
\begin{algorithmic}
\REQUIRE ~~\\%Input
The sources $S$, dictionary $\Phi$, number of morphological components $N$, number of iterations $L_{max}$ and threshold $\delta^{(0)} = k \cdot L_{max}$.
\ENSURE ~~\\ %Output
Estimation $\hat{\mathbf{A}}$ and $\hat{\mathbf{S}}$.

\STATE 1. Initialse $\mathbf{\Phi}$ to a known overcomplete dictionary.\\
\STATE 2. Set $\mathbf{A}$ to a radom column-normalised matrix.\\
\STATE 3. $\mathbf{X} = \mathbf{A}^T\mathbf{Y}$.\\

\STATE 4. for $L_{max}$ iterations:\\
\quad \quad for $j = 1:N$:\\
\quad \quad \quad - Extract patches from $x_j$.\\
\quad \quad \quad - Fix dictionary $\mathbf{\Phi}^{(m-1)}$ and update coefficients $\alpha^{m}$ and block structure $d^{(m)}$ by applying sparse agglomerative clustering.\\
\quad \quad \quad- Fix the block structure $d^{(m)}$ and update the dictionary atoms $\mathbf{\Phi}^{(m)}$ by applying BK-SVD.\\
\quad \quad \quad - Calculate the residual $\mathbf{E_j} = \mathbf{Y} - \sum_{l\neq j}a_lx_l^T$. \\
\quad \quad \quad - Compute $x_j$. \\
\quad \quad \quad - Calculate mixing matrix column $a_j = \mathbf{E_j}x_j$ \\
\quad \quad \quad - Normalise $a_j$  \\
\STATE 5. Decrease $\sigma$ until stopping criterion is met. \\
\end{algorithmic}
\end{algorithm}


\subsection{Complexity Analysis}
The standard algorithm for agglomerative clustering (SAC) has a time complexity of ${\displaystyle {\mathcal {O}}(K^{3})}$ (K is the number of dictionary atoms in this occasion). This makes it not suitable for even medium datasets. However the complexity of BK-SVD requires $s$ times less number of singular value computations than the K-SVD algorithm. Because of the simultaneous updating of atoms belong to the same block.
As proved in \cite{OMP_KSVD}, K-SVD algorithm has a complexity of $\displaystyle {\mathcal {O}}((ks)^2K + 2NK)$. Because of the block nature of BK-SVD, we expect BK-SVD to have a complexity of $\displaystyle {\mathcal {O}}((k)^2K + 2NK)$, where $k$ is the sparsity level (number of sparse blocks) and $s$ is the maximal block size. The total compexity of combined SAC+BK-SVD is therefore $\displaystyle {\mathcal {O}}((k)^2K + 2NK+K^3)$. Therefore the overall convergence rate of the proposed BSS framework will be slower than simple K-SVD method.\\

