\section{Experiments on Image Source Separation}
\subsection{Software}
The software in this project is developed based on several opensource Matlab Toolboxes (i.e. WaveLab 850, K-SVD, MCALab110). Complete version of my code can be found on Github\footnote{https://github.com/Dieselmarble/FYP}. In addition, I wrote my own BS-KSVD Toolbox which involves considerable novelty.

\subsection{Image Decomposition}
In this section, we turn to use MMCA to separate two-dimensional data and compare the result with the standard ICA source separation techniques. \\

In Figure \ref{segmentation_Im} (a) are two source signals, one of which is oscillating textures while another is  a `boy' image. Curvelet transform is selected as the dictionary for source 1 and discrete consine transform is selected for source 2. This is similar to adpot the idea of a double sparse dictionary therefore we can decompose the mixtures into cartoon and texture. Figure \ref{segmentation_Im} (c) illustrates the separated image using MCA under the presence of 20dB gaussian noise. It can be shown that MCA is able to split the texture and cartoon parts. However, the reconstruction quality of the `boy' image using curvelet dictionary in MCA does not give satisfactory result though. We think the decomposition presenting in the dictionary domain may not be extremely sparse, as some of the mixtures can still be seen in the output image. \\

Note that the MCA algorithm, unlike BSS methods, only takes one single combination of sources ($m = 1$). Now we extend the observed mixtures to a multichannel case and BSS techniques applies. The correlation coefficient between two sources are only 0.07. Hence the independence assumption for ICA methods is valid here. We create four mixtures from two source images.
\ref{segmentation_Im} (d) shows that MMCA is clearly able to efficiently separate the original source images, achieving better visual results than FastICA in (b). Quantitatively, Figure \ref{segmen_1} shows the correlation between the original sources and those estimated. As the data noise variance increase, 
MMCA (dashed line) clearly achieves better estimation quality and shows clear robustness compared to non de-noised ICA methods. In addition, one can note that both JADE and FastICA provides similar performance.\\

Figure \ref{segmen_1} plots the matrix estimation error is defined as $||\mathbf{I_n} - P\tilde{\mathbf{A}}^{+}\mathbf{A} ||$, after elimination the effect of the permutation and scale indeterminacy. Contrasting with standard ICA methods, MMCA iteratively estimates the mixing matrix from coarse (i.e., smooth) versions of the sources and thus is not penalized by the presence of noise. As a consequence, MMCA is clearly more robust to noise than standard ICA methods, even under very noisy context \cite{BobinJ_2006Mdas}. This result answers our expectation in section \ref{ica_defect} that ICA is not robust under the additive Gaussian nosie setting.\\

The results in this experiment proves that sparsity based methods successfully handle the image segmentation task. Especially under the multichannel case, MMCA significantly outperforms the standard ICA methods in terms of separation quality and robustness.\\




% -------------------------
\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/source1.png}\vspace{1pt}
\includegraphics[width=1\linewidth]{images/source2.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/separated1.png}\vspace{1pt}
\includegraphics[width=1\linewidth]{images/separated2.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/MCA_Cartoon.png}\vspace{1pt}
\includegraphics[width=1\linewidth]{images/MCA_Texture.png}
\end{minipage}} 
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/mmca_cartoon.png}\vspace{1pt}
\includegraphics[width=1\linewidth]{images/mmca_texture.png}
\end{minipage}}
\caption{Experiment1: Image segmentation (PSNR = 20dB); \textbf{(a)}: Original sources; \textbf{(b)}: FastICA outputs; \textbf{(c)}: MCA outputs; \textbf{(d)}: MMCA outputs}
\label{segmentation_Im}
\end{figure}
% -------------------------


\begin{figure}[!htbp]
\centering
\begin{minipage}[b]{0.49\textwidth}
\includegraphics[width=\textwidth]{images/corr_plot_seg.png}
\caption{Evolution of the correlation coefficient between original and estimated sources as the noise variance varies.}
\label{segmen_1}
\end{minipage}
\begin{minipage}[b]{0.49\textwidth}
\includegraphics[width=\textwidth]{images/mmc_plot_seg.png}
\caption{Evolution of the mixing matrix criterion (after indeterminacy corrected) as the noise variance varies.}
\end{minipage}
\label{segmen_2}
\label{imapint1}
\end{figure}

\newpage
\subsection{Blind Image Source Separation}
In this experiment, we mix 4 pictures into 10 channels. The source images are picked as they contains similar morphologies. Classical ICA methods and MMCA, GMCA are applied to separate the source. Unfortunately the FastICA methods is not able to separate the original source. We hence adopt a sophisticated variant of it abbreviated as EFICA. Moreover, it has been proved in \cite{BobinJ_2007SaMD} that using a single overcomplete DWT dictionary or a union of DCT and DWT dictionary in GMCA provides similar results. We therefore use a discrete wavelet dictionary in FastGMCA (FGMCA).\\


GMCA has been proved elsewhere \cite{BobinJ_2007SaMD} that it achieves better separation quality when the estimated components are 'very' sparse in the given dictionary. Therefore GMCA is supposed to perform well on separating images even without distinct discrepancies. Separated images are displayed in Figure (\ref{BSS_ex2_11}). It shows the recovered image by various BSS methods contaminated by 20dB noise. All methods can distinguish the barbara image (row 3) from the mixtures. The ICA methods, nonetheless fail to separate the scenery photos (row 1 and 2). But in (e) using GMCA still gives acceptable result. MMCA using curvelet + DCT in (d) gives better result than ICA methods, but not so good as GMCA.\\


% ---------- begin figure-------------
\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{minipage}[b]{0.17\linewidth}
\includegraphics[width=1\linewidth]{images/boat_ori.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/paraty_ori.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/barbara_ori.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/pakhawaj_ori.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.17\linewidth}
\includegraphics[width=1\linewidth]{images/efica_out1.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/efica_out2.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/efica_out3.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/efica_out4.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.17\linewidth}
\includegraphics[width=1\linewidth]{images/jade_out1.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/jade_out2.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/jade_out3.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/jade_out4.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.17\linewidth}
\includegraphics[width=1\linewidth]{images/mmca_ex24.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/mmca_ex22.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/mmcaex21.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/mmca_ex23.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.17\linewidth}
\includegraphics[width=1\linewidth]{images/gmca_out1.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/gmca_out2.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/gmca_out3.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/gmca_out4.png}
\end{minipage}}
\caption{Experiment2: image separation (20dB noise); \textbf{(a)}:Original image sources; \textbf{(b)}:EFICA outputs; \textbf{(c)}:JADE outputs; \textbf{(d)}:MMCA outputs; \textbf{(2)}:GMCA outputs.}
\label{BSS_ex2_11}
\end{figure}

% ----------end figure------------- %

Figure (\ref{BSS_EV21}) portrays the evolution of average correlation coefficient over 4 estimated sources as a funciion of the noise variance. At a first glance, GMCA significantly outperforms the ICA methods interms of robustness and separation quality. JADE performs the worst in among all ICA based algorithms. Figure (\ref{BSS_EV21}) depicts the behavior of the imxing matrix criterion as the noise decreases. The mixing matrix criterion also clearly revels the strength of GMCA method.\\

% ----------begin figure------------- %
\begin{figure}[!htbp]
\centering
\begin{minipage}[b]{0.49\textwidth}
\includegraphics[width=\textwidth]{images/image_separation1.png}
\caption{Evolution of the correlation coefficient between original and estimated sources as the noise variance varies.}
\label{BSS_EV21}
\end{minipage}
\begin{minipage}[b]{0.49\textwidth}
\includegraphics[width=\textwidth]{images/image_separation2.png}
\caption{Evolution of the mixing matrix criterion (after indeterminacy corrected) as the noise variance varies.}
\label{BSS_EV22}
\end{minipage}
\end{figure}
% ----------end figure------------- %

To summarise the findings in this experiment, GMCA does take good advantage of overcompleteness and morphological diversity. We can aslo claim that sparsity brings better results. Taking the advantages further, in next experiment, we explore how locally learned dictionary helps the blind source separation.\\


\clearpage
\subsection{Blind image separation using adaptive dictionary learning}
In this experiment, we illustrate the results of applying the K-SVD BSS algorithm and the block-sparse BSS algorithm. Same as last experiment settings, a severe case of 4 image sources with very different morphologies was chosen to exam the performance of the methods. 400 iterations were selected as the stopping criterion, addive gaussian noise is added from 3dB to 40dB. The wavelet based GMCA is selected as the baseline algorithm.\\

In order to obtain enough training samples for dictionary learning, multiple overlapped segments (patches) of the sources are taken \cite{VAbolghasemi2012}. Choose the optimal patch size is a subtle problem. Generally, very large patches should be avoided as they lead to massive dictionaries and also provide few training samples for the dictionary learning stage. We choose $8\times 8$ patches for this experiment. Furthermore the patches are $50\%$ overlapped as suggested in \cite{VAbolghasemi2012}.\\

The dictionaries obtained have size of $64\times 256$. The plots in (\ref{dictionary_learned}) have $16 \times 16 = 256$ blocks whereas each block is obtained from a $8\times 8$ patches. Figure (\ref{dictionary_learned}) also proves both the learned local dictionaries have good adaption to the corresponding sources, and significantly differnent from the standard DCT dictionary in (a).\\

\begin{figure}[!htbp]
\centering
\subfigure[]{
\begin{minipage}[b]{0.99\linewidth}
\centering
\includegraphics[width=0.23\linewidth]{images/DCT_dict.png}\vspace{4pt}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.99\linewidth}
\includegraphics[width=0.23\linewidth]{images/dic1.png}\vspace{4pt}
\includegraphics[width=0.23\linewidth]{images/dict3.png}\vspace{4pt}
\includegraphics[width=0.23\linewidth]{images/dic2.png}\vspace{4pt}
\includegraphics[width=0.23\linewidth]{images/dict4.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.99\linewidth}
\includegraphics[width=0.23\linewidth]{images/block_dict1.png}\vspace{4pt}
\includegraphics[width=0.23\linewidth]{images/blockdict2.png}\vspace{4pt}
\includegraphics[width=0.23\linewidth]{images/blockdict3.png}\vspace{4pt}
\includegraphics[width=0.23\linewidth]{images/blockdict4.png}
\end{minipage}}
% \vspace{-0.5cm} 
\caption{(a): A standard discrete cosine (DCT) dictionary; (b): Trained dictionary using K-SVD for sources 1,2,3,4 respectively; (b): Trained dictionary using SAC+BK-SVD for the 4 sources.}
\label{dictionary_learned}
\end{figure}

Figure (\ref{apt_compare}) compares the separation results
using several algorithm under 20dB Gaussian noise. \\


% ---------begin figure ------------ %
\begin{figure*}[!htbp]
\centering
\subfigure[]{
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/boat_ori.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/paraty_ori.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/barbara_ori.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/pakhawaj_ori.png}
\end{minipage}}
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/gmca_out1.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/gmca_out2.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/gmca_out3.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/gmca_out4.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/ammca_out4.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/ammca_out2.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/ammca_out3.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/ammca_out1.png}
\end{minipage}}
\subfigure[]{
\begin{minipage}[b]{0.23\linewidth}
\includegraphics[width=1\linewidth]{images/bk3.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/bk1.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/bk2.png}\vspace{4pt}
\includegraphics[width=1\linewidth]{images/bk4.png}
\end{minipage}}
\caption{Experiment 3 (20dB): Using adaptive dictionary learning; \textbf{(a)}: original sources; \textbf{(b)}:GMCA outputs; \textbf{(c)}:K-SVD+MMCA output; \textbf{(d)}:BK-SVD+SAC+MMCA outputs;}
\label{apt_compare}
\end{figure*}
% ---------end figure ------------ %

The running time for K-SVD BSS algorithm is 221 seconds. However the running time for BK-SVD BSS algorithm is 389 seconds. For Fast GMCA it is only 4 seconds. This coincides our deduction before, the sparse agglomerative clustering algorithm dominates the complexity. Even though the BK-SVD algorithm is supposed to be faster than K-SVD, but the clustering stage slows down the overall process. It is also seen that, due to learning the dictionary, making both algorithms are computationally demanding for large scale applications (i.e. image processing) and are much slower than the GMCA algorithm \\
% --------------begin table ----------- %
\begin{table}[!htbp]
\centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    MMCA & Fast GMCA & K-SVD & BK-SVD+SAC \\\hline
    $221s$ & $4s$ & $221s$ & $389s$ \\\hline
    \end{tabular}
    \caption{Running time for GMCA, KSVD and SAC+BK-SVD}
\end{table}
% --------------end table ----------- %
 